"""ChromaDB vector store for semantic card search."""

from pathlib import Path
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pickle


class TFIDFEmbeddingFunction:
    """Simple TF-IDF based embedding function that runs completely locally."""

    def __init__(self):
        self.name = "tfidf"
        self.vectorizer = TfidfVectorizer(
            max_features=384,  # Match typical embedding dimension
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.is_fitted = False
        self.documents_cache = []

    def __call__(self, input: List[str]) -> List[List[float]]:
        """Generate embeddings for input texts."""
        # Cache documents for fitting
        self.documents_cache.extend(input)

        if not self.is_fitted and self.documents_cache:
            # Fit on first batch
            self.vectorizer.fit(self.documents_cache)
            self.is_fitted = True

        if self.is_fitted:
            # Transform documents to vectors
            vectors = self.vectorizer.transform(input).toarray()
        else:
            # Return zero vectors if not fitted yet
            vectors = np.zeros((len(input), 384))

        return vectors.tolist()


class VectorStore:
    """Manages card embeddings and semantic search using ChromaDB."""

    PERSIST_DIR = Path("data/chroma_db")
    COLLECTION_NAME = "mtg_cards"
    VECTORIZER_PATH = Path("data/vectorizer.pkl")

    def __init__(self):
        """Initialize ChromaDB client and collection."""
        self.PERSIST_DIR.mkdir(parents=True, exist_ok=True)

        # Initialize ChromaDB with persistence
        self.client = chromadb.PersistentClient(
            path=str(self.PERSIST_DIR),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        # Load or initialize TF-IDF vectorizer
        if self.VECTORIZER_PATH.exists():
            print("Loading saved vectorizer...")
            with open(self.VECTORIZER_PATH, 'rb') as f:
                self.vectorizer = pickle.load(f)
            self.is_vectorizer_fitted = True
        else:
            self.vectorizer = TfidfVectorizer(
                max_features=384,
                stop_words='english',
                ngram_range=(1, 2)
            )
            self.is_vectorizer_fitted = False

        # Get or create collection
        self.collection = self.client.get_or_create_collection(
            name=self.COLLECTION_NAME,
            metadata={"description": "Magic: The Gathering card embeddings", "hnsw:space": "cosine"}
        )

    def _prepare_card_text(self, card: Dict[str, Any]) -> str:
        """
        Prepare searchable text from card data.

        Args:
            card: Card dictionary from Scryfall

        Returns:
            Formatted text for embedding
        """
        parts = []

        # Card name
        if "name" in card:
            parts.append(f"Name: {card['name']}")

        # Type line
        if "type_line" in card:
            parts.append(f"Type: {card['type_line']}")

        # Oracle text (rules text)
        if "oracle_text" in card:
            parts.append(f"Text: {card['oracle_text']}")

        # Mana cost
        if "mana_cost" in card:
            parts.append(f"Cost: {card['mana_cost']}")

        # Power/Toughness for creatures
        if "power" in card and "toughness" in card:
            parts.append(f"P/T: {card['power']}/{card['toughness']}")

        # Keywords
        if "keywords" in card and card["keywords"]:
            parts.append(f"Keywords: {', '.join(card['keywords'])}")

        return " | ".join(parts)

    def _prepare_metadata(self, card: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract relevant metadata from card.

        Args:
            card: Card dictionary from Scryfall

        Returns:
            Metadata dictionary
        """
        metadata = {
            "name": card.get("name", ""),
            "type_line": card.get("type_line", ""),
            "set": card.get("set", ""),
            "rarity": card.get("rarity", ""),
        }

        # Add colors if available
        if "colors" in card:
            metadata["colors"] = ",".join(card["colors"])

        # Add mana cost if available
        if "mana_cost" in card:
            metadata["mana_cost"] = card["mana_cost"]

        # Add CMC if available
        if "cmc" in card:
            metadata["cmc"] = str(int(card["cmc"]))

        return metadata

    def upsert_cards(self, cards: List[Dict[str, Any]], batch_size: int = 1000) -> None:
        """
        Insert or update cards in the vector database.

        Args:
            cards: List of card dictionaries from Scryfall
            batch_size: Number of cards to process per batch
        """
        total = len(cards)
        print(f"Upserting {total} cards into ChromaDB...")

        # First pass: collect all documents for fitting vectorizer
        all_docs = []
        for card in cards:
            if card.get("id"):
                all_docs.append(self._prepare_card_text(card))

        # Fit vectorizer on all documents
        print("  Fitting TF-IDF vectorizer on card texts...")
        self.vectorizer.fit(all_docs)
        self.is_vectorizer_fitted = True

        # Save vectorizer for later use
        print("  Saving vectorizer...")
        with open(self.VECTORIZER_PATH, 'wb') as f:
            pickle.dump(self.vectorizer, f)

        # Second pass: process and add cards in batches
        for i in range(0, total, batch_size):
            batch = cards[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total + batch_size - 1) // batch_size

            # Prepare data for ChromaDB
            ids = []
            documents = []
            metadatas = []
            embeddings = []

            for card in batch:
                # Use Scryfall ID as unique identifier
                card_id = card.get("id")
                if not card_id:
                    continue

                doc_text = self._prepare_card_text(card)
                ids.append(card_id)
                documents.append(doc_text)
                metadatas.append(self._prepare_metadata(card))

                # Compute embedding locally
                embedding = self.vectorizer.transform([doc_text]).toarray()[0].tolist()
                embeddings.append(embedding)

            # Add batch with pre-computed embeddings
            if ids:
                self.collection.add(
                    ids=ids,
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas
                )

            print(f"  Batch {batch_num}/{total_batches} complete ({len(ids)} cards)")

        print(f"âœ“ Upserted {total} cards successfully")

    def query_similar(self, query: str, n_results: int = 5) -> Dict[str, Any]:
        """
        Search for cards similar to the query.

        Args:
            query: Natural language search query
            n_results: Number of results to return

        Returns:
            Dictionary with 'ids', 'documents', 'metadatas' keys
        """
        if not self.is_vectorizer_fitted:
            raise RuntimeError("Vectorizer not fitted. Please run upsert_cards first.")

        # Compute query embedding locally
        query_embedding = self.vectorizer.transform([query]).toarray()[0].tolist()

        # Query with pre-computed embedding
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )

        return results

    def count(self) -> int:
        """
        Get the total number of cards in the database.

        Returns:
            Number of cards stored
        """
        return self.collection.count()
